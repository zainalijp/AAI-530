{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Analytics: Preprocessing Data for Tableau \n",
    "-----------------------------------------------------\n",
    "Because our dataset is incredibly large, our Tableau dashboard, representing the user interface, will focus on individual user data. In order to also gain a greater understanding of the combined user data from a backend business analytics standpoint, we have decided to include a secondary dashboard representing the adminstrative interface. The following code snippet includes several preprocessing steps to extract the necessary information for our business interface without the computational intensity of including the entire raw dataset in Tableau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries \n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's combine the dataset, add 'user_id' as a column to retain individual data and use the assumption of negatives to fill in any missing values in the ground truth labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 377346 entries, 0 to 377345\n",
      "Columns: 279 entries, timestamp to user_id\n",
      "dtypes: float64(276), int64(2), object(1)\n",
      "memory usage: 803.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Accessing copy of stored files\n",
    "directory = Path('Users/halladaykinsey/Desktop/ExtraSensory.per_uuid_features_labels-Unzipped copy')\n",
    "dfs = []\n",
    "\n",
    "# Iterating over each file in the directory\n",
    "for file in directory.glob('*.csv'):\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Adding user_id as column\n",
    "    user_id = file.stem  \n",
    "    df['user_id'] = user_id\n",
    "    \n",
    "    # Cleaning ground truth columns\n",
    "    label_cols = df.columns[df.columns.str.startswith('label')]\n",
    "    df[label_cols] = df[label_cols].fillna(0)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(combined_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's find the user with the most and least number of missing values. In our business interface, this will be helpful for administrators to understand which users are missing specific sensors and which user prediction would be more accurate based on the number of sensor values being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User with the most missing values: 86A4F379-B305-473D-9D83-FC7D800180EF.features_labels - 765987 missing values\n",
      "User with the least missing values: C48CE857-A0DD-4DDB-BEA5-3A25449B2153.features_labels - 50267 missing values\n"
     ]
    }
   ],
   "source": [
    "# Calculating missing values within input sensors for each user\n",
    "input_cols = combined_df.columns[~combined_df.columns.str.startswith(('label', 'timestamp', 'user_id'))]\n",
    "combined_df['missing_values_count'] = combined_df[input_cols].isnull().sum(axis=1)\n",
    "missing_values_per_user = combined_df.groupby('user_id')['missing_values_count'].sum()\n",
    "\n",
    "# Identifying users with the most and least missing values\n",
    "user_with_most_missing_values = missing_values_per_user.idxmax()\n",
    "user_with_least_missing_values = missing_values_per_user.idxmin()\n",
    "\n",
    "print(f\"User with the most missing values: {user_with_most_missing_values} - {missing_values_per_user.max()} missing values\")\n",
    "print(f\"User with the least missing values: {user_with_least_missing_values} - {missing_values_per_user.min()} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1446141691\n",
      "1    1446141752\n",
      "2    1446141805\n",
      "3    1446141873\n",
      "4    1446141925\n",
      "Name: timestamp, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Understanding timestamp to convert to datetime\n",
    "print(combined_df['timestamp'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             user_id          start_time\n",
      "0  00EABED2-271D-49D8-B599-1D4A09240601.features_... 2015-10-05 21:06:01\n",
      "1  098A72A5-E3E5-4F54-A152-BBDA0DF7B694.features_... 2015-08-04 17:14:18\n",
      "2  0A986513-7828-4D53-AA1F-E02D6DF9561B.features_... 2015-12-08 19:06:37\n",
      "3  0BFC35E2-4817-4865-BFA7-764742302A2D.features_... 2015-10-20 18:42:14\n",
      "4  0E6184E1-90C0-48EE-B25A-F1ECB7B9714E.features_... 2015-11-30 18:10:05\n"
     ]
    }
   ],
   "source": [
    "# Converting Unix timestamps to datetime format\n",
    "combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'], unit='s')\n",
    "\n",
    "# Finding 'date of join' for each user\n",
    "user_start_times = combined_df.groupby('user_id')['timestamp'].min().reset_index()\n",
    "user_start_times.rename(columns={'timestamp': 'start_time'}, inplace=True)\n",
    "print(user_start_times.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully to desktop\n"
     ]
    }
   ],
   "source": [
    "# Saving to CSV \n",
    "user_start_times.to_csv('/Users/halladaykinsey/Desktop/user_start_times.csv', index=False)\n",
    "print(\"File saved successfully to desktop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have calculated the users with the most and least missing values, we can clean the rest of the input variables according to the threshold we used for our models in order to calculate similarity between users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 191 columns out of 280\n"
     ]
    }
   ],
   "source": [
    "# Calculating the percentage of missing data for each feature\n",
    "total_nan_counts = combined_df.isnull().sum()\n",
    "total_entries_per_feature = len(combined_df)\n",
    "percentage_missing = (total_nan_counts / total_entries_per_feature) * 100\n",
    "\n",
    "# Removing all input columns with missing values to maintain consistency with processed models\n",
    "threshold = 0\n",
    "columns_to_remove = percentage_missing[percentage_missing > threshold].index.tolist()\n",
    "print(f\"Removing {len(columns_to_remove)} columns out of {len(combined_df.columns)}\")\n",
    "combined_df.drop(columns=columns_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two most similar users are: ('11B5EC4D-4133-4289-B475-4E737182A406.features_labels', '2C32C23E-E30C-498A-8DD2-0EFB9150A02E.features_labels') with a distance of 0.49951046741197547\n"
     ]
    }
   ],
   "source": [
    "# Sorting through dataframe for included values \n",
    "data_for_similarity = combined_df.select_dtypes(include=['float64', 'int64', 'bool'])\n",
    "\n",
    "# Calling unique user IDs\n",
    "user_ids = combined_df['user_id'].unique()\n",
    "\n",
    "\n",
    "min_distance = np.inf\n",
    "min_users = (None, None)\n",
    "\n",
    "# Iterating over each pair of users to calculate the Euclidean distance\n",
    "for i in range(len(user_ids)):\n",
    "    for j in range(i + 1, len(user_ids)):\n",
    "        user1_data = data_for_similarity[combined_df['user_id'] == user_ids[i]]\n",
    "        user2_data = data_for_similarity[combined_df['user_id'] == user_ids[j]]\n",
    "        \n",
    "        # Aligning data with columns\n",
    "        if not user1_data.columns.equals(user2_data.columns):\n",
    "            print(f\"Skipping users {user_ids[i]} and {user_ids[j]} due to different columns\")\n",
    "            continue\n",
    "\n",
    "        # Calculating Euclidean distance\n",
    "        dist = euclidean(user1_data.mean(), user2_data.mean())\n",
    "        \n",
    "        if dist < min_distance:\n",
    "            min_distance = dist\n",
    "            min_users = (user_ids[i], user_ids[j])\n",
    "\n",
    "print(f\"The two most similar users are: {min_users} with a distance of {min_distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for user 11B5EC4D-4133-4289-B475-4E737182A406.features_labels saved to: /Users/halladaykinsey/Desktop/user_11B5EC4D-4133-4289-B475-4E737182A406.features_labels_data.csv\n",
      "Data for user 2C32C23E-E30C-498A-8DD2-0EFB9150A02E.features_labels saved to: /Users/halladaykinsey/Desktop/user_2C32C23E-E30C-498A-8DD2-0EFB9150A02E.features_labels_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Extracting the two most similar user data and saving to csv\n",
    "user1_data = combined_df[combined_df['user_id'] == min_users[0]]\n",
    "user2_data = combined_df[combined_df['user_id'] == min_users[1]]\n",
    "\n",
    "user1_data.to_csv(f'/Users/halladaykinsey/Desktop/user_{min_users[0]}_data.csv', index=False)\n",
    "user2_data.to_csv(f'/Users/halladaykinsey/Desktop/user_{min_users[1]}_data.csv', index=False)\n",
    "\n",
    "print(f\"Data for user {min_users[0]} saved to: /Users/halladaykinsey/Desktop/user_{min_users[0]}_data.csv\")\n",
    "print(f\"Data for user {min_users[1]} saved to: /Users/halladaykinsey/Desktop/user_{min_users[1]}_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
