{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/r4wkb8dj54b5k6_vd_8stbq00000gn/T/ipykernel_82243/2270704948.py:10: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pickle\n",
    "from threading import Timer\n",
    "\n",
    "# Data handling and numerical analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Machine Learning and Deep Learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten, BatchNormalization, Reshape, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator\n",
    "from tensorflow.keras.models import load_model, clone_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Consolidation\n",
    "This section focuses on the process of combining data from multiple sources into a single, unified dataset. It involves unzipping data files, extracting CSV files from the zipped archives, and merging these CSV files. The goal is to create a comprehensive dataset that facilitates easier data manipulation and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure ExtraSensory.per_uuid_features_labels.zip exists and is unziped\n",
    "def unzip(zip_file):\n",
    "    # Extract to the directory obtained from the zip file name\n",
    "    zip_extract_to = zip_file.replace('.zip', '')\n",
    "\n",
    "    # Unzipping\n",
    "    if os.path.exists(zip_file):\n",
    "        if not os.path.exists(zip_extract_to):\n",
    "            os.makedirs(zip_extract_to)\n",
    "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(zip_extract_to)\n",
    "            message = \"Unzipped successfully.\"\n",
    "        else:\n",
    "            message = \"Directory already exists. File might be unzipped.\"\n",
    "    else:\n",
    "        message = \"Zip file not found.\"\n",
    "\n",
    "    print(message)\n",
    "    return zip_extract_to\n",
    "\n",
    "\n",
    "def csv_extract(zip_extract_to):\n",
    "    # Improved variable name for the directory where the extracted files will be saved\n",
    "    unzipped_data_dir = f\"{zip_extract_to}-Unzipped\"\n",
    "\n",
    "    # Create the unzipped data directory if it does not exist\n",
    "    if not os.path.exists(unzipped_data_dir):\n",
    "        os.makedirs(unzipped_data_dir)\n",
    "\n",
    "    # Extracting .csv.gz files\n",
    "    extraction_message = \"\"\n",
    "    if os.path.exists(zip_extract_to):\n",
    "        for file in os.listdir(zip_extract_to):\n",
    "            if file.endswith('.gz'):\n",
    "                gz_file_path = os.path.join(zip_extract_to, file)\n",
    "                csv_file_path = os.path.join(unzipped_data_dir, file[:-3])  # Removing '.gz' from filename\n",
    "\n",
    "                try:\n",
    "                    with gzip.open(gz_file_path, 'rb') as f_in:\n",
    "                        with open(csv_file_path, 'wb') as f_out:\n",
    "                            shutil.copyfileobj(f_in, f_out)\n",
    "                    extraction_message += f\"Extracted {file}\\n\"\n",
    "                except Exception as e:\n",
    "                    extraction_message += f\"Error extracting {file}: {e}\\n\"\n",
    "    else:\n",
    "        extraction_message = \"Directory with .gz files not found.\"\n",
    "\n",
    "    print(extraction_message.strip())\n",
    "\n",
    "    return unzipped_data_dir\n",
    "\n",
    "\n",
    "# Function to extract user_id from filename\n",
    "def extract_user_id(filename):\n",
    "    return filename.split('.')[0]\n",
    "\n",
    "def make_one_csv(unzipped_data_dir, COMBINED_FILE):\n",
    "    # Combining all CSVs into one dataframe\n",
    "    combined_csv_data = pd.DataFrame()\n",
    "\n",
    "    if os.path.exists(unzipped_data_dir):\n",
    "        for file in os.listdir(unzipped_data_dir):\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(unzipped_data_dir, file)\n",
    "                user_id = extract_user_id(file)\n",
    "\n",
    "                # Read the CSV file and add the user_id column\n",
    "                csv_data = pd.read_csv(file_path)\n",
    "                csv_data['user_id'] = user_id\n",
    "\n",
    "                # Append to the combined dataframe\n",
    "                combined_csv_data = pd.concat([combined_csv_data, csv_data], ignore_index=True)\n",
    "\n",
    "                \n",
    "                #print(f\"Processed file: {file} \\nCurrent size of combined data: {combined_csv_data.shape}\")\n",
    "\n",
    "\n",
    "        # Check if any data has been combined\n",
    "        if not combined_csv_data.empty:\n",
    "            # Save the combined CSV data to a file\n",
    "            combined_csv_data.to_csv(COMBINED_FILE, index=False)\n",
    "            print(f\"Combined CSV file created at {COMBINED_FILE}.\")\n",
    "        else:\n",
    "            print(\"No CSV files found to combine or combined data is empty.\")\n",
    "    else:\n",
    "        print(\"Directory with unzipped CSV files not found.\")\n",
    "    return COMBINED_FILE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_FILE = 'ExtraSensory_Combined_User_Data.csv'\n",
    "if not os.path.exists(COMBINED_FILE):\n",
    "    # Path of the zip file\n",
    "    zip_file = 'ExtraSensory.per_uuid_features_labels.zip'\n",
    "    zip_extract_to = unzip(zip_file)\n",
    "    unzipped_data_dir = csv_extract(zip_extract_to)\n",
    "    make_one_csv(unzipped_data_dir, COMBINED_FILE)\n",
    "else:\n",
    "    print('Combined file already exists.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Models Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv_data = pd.read_csv(COMBINED_FILE)\n",
    "combined_csv_data['timestamp'] = pd.to_datetime(combined_csv_data['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_csv_data.columns)\n",
    "\n",
    "# user_id is for us to make sure we have record on source of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to understand columns\n",
    "def build_hierarchy(columns):\n",
    "    # Build a nested dictionary representing the hierarchy of columns.\n",
    "    hierarchy = {}\n",
    "    for col in columns:\n",
    "        parts = col.split(':')\n",
    "        current_level = hierarchy\n",
    "\n",
    "        for part in parts[:-1]:\n",
    "            current_level = current_level.setdefault(part, {})\n",
    "        \n",
    "        current_level[parts[-1]] = col\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "def format_hierarchy(hierarchy, indent=0):\n",
    "    # Format the hierarchy into a readable string with indentation.\n",
    "    result = \"\"\n",
    "    for key, value in hierarchy.items():\n",
    "        prefix = \"  \" * indent + \"- \"\n",
    "        if isinstance(value, dict):\n",
    "            result += f\"{prefix}{key}:\\n{format_hierarchy(value, indent + 1)}\"\n",
    "        else:\n",
    "            result += f\"{prefix} {key}\\n\"\n",
    "    return result\n",
    "\n",
    "# Building and formatting the hierarchy\n",
    "hierarchy = build_hierarchy(combined_csv_data.columns)\n",
    "formatted_hierarchy = format_hierarchy(hierarchy)\n",
    "print(formatted_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of label columns to check\n",
    "label_columns = [col for col in combined_csv_data.columns if col.startswith(\"label:\")]\n",
    "\n",
    "# Assumption of negatives for ground truths\n",
    "combined_csv_data[label_columns] = combined_csv_data[label_columns].fillna(0)\n",
    "\n",
    "combined_csv_data['label_sum_inital'] = combined_csv_data[label_columns].sum(axis=1)\n",
    "combined_csv_data['label:UNKNOWN'] = (combined_csv_data['label_sum_inital'] == 0).astype(float)\n",
    "label_columns.append('label:UNKNOWN')\n",
    "combined_csv_data = combined_csv_data.drop('label_sum_inital', axis=1)\n",
    "df = combined_csv_data.copy()\n",
    "\n",
    "\n",
    "# Function to find the label name with value 1\n",
    "def find_label_name(row):\n",
    "    for col in label_columns:\n",
    "        if row[col] == 1:\n",
    "            return col.split(\"label:\")[1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking ground truth labels for value counts\n",
    "column_sums = df[label_columns].sum()\n",
    "column_sums_sorted = column_sums.sort_values(ascending=True)\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(10, 14)) \n",
    "column_sums_sorted.plot(kind='barh')\n",
    "plt.title('Sum of Each Column in DataFrame')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Most Done Activities')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing Important Columns\n",
    "unneeded_columns = ['user_id', 'label_source']\n",
    "output_columns = [col for col in combined_csv_data.columns if col.startswith('label:')]\n",
    "input_columns = [col for col in combined_csv_data.columns if col not in output_columns and col not in unneeded_columns]\n",
    "X_main = df.copy()[input_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking input variables for missing values\n",
    "def nan_percentage(df):\n",
    "    nan_percentage = (df.isna().mean() * 100).round(2)\n",
    "    nan_percentage_df = pd.DataFrame({'Variable': nan_percentage.index, 'NaN Percentage': nan_percentage.values})\n",
    "    nan_percentage_df = nan_percentage_df.sort_values(by='NaN Percentage', ascending=True)\n",
    "    \n",
    "    # Plot setup\n",
    "    plt.figure(figsize=(8, 35)) \n",
    "    plt.barh(nan_percentage_df['Variable'], nan_percentage_df['NaN Percentage'])\n",
    "    plt.xlabel('Percentage of Missing Values')\n",
    "    plt.ylabel('Variables')\n",
    "    plt.title('Missing Value Percentage by Variable')\n",
    "    plt.grid(axis='x')\n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "    return nan_percentage_df\n",
    "\n",
    "# Checking overall missing percentage\n",
    "nan_percentage_df = nan_percentage(X_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_users = df.drop(columns=['label_source'])\n",
    "users = X_with_users['user_id'].unique()\n",
    "features = input_columns\n",
    "\n",
    "# Initialize a list to store the counts for each user\n",
    "nan_counts_list = []\n",
    "\n",
    "# Loop through each user\n",
    "for user in users:\n",
    "    # Filter the DataFrame for the current user\n",
    "    df_user = X_with_users[X_with_users['user_id'] == user]\n",
    "    \n",
    "    # Count the NaN values for each feature for the current user and add user_id to the series\n",
    "    nan_count = df_user[features].isna().sum()\n",
    "    nan_count['user_id'] = user  # Add user_id to the count\n",
    "    \n",
    "    # Append the count series to the list\n",
    "    nan_counts_list.append(nan_count)\n",
    "\n",
    "# Convert the list of Series to a DataFrame\n",
    "nan_counts_per_user = pd.DataFrame(nan_counts_list)\n",
    "\n",
    "# If needed, set the user_id as the index\n",
    "nan_counts_per_user.set_index('user_id', inplace=True)\n",
    "\n",
    "\n",
    "# Plot the heat map\n",
    "plt.figure(figsize=(30, 30))\n",
    "sns.heatmap(nan_counts_per_user, annot=False, cmap='Reds')\n",
    "plt.title('Heatmap of Missing Values per User')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Users')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of rows for each user in the original DataFrame\n",
    "user_total_length = X_with_users.groupby('user_id').size()\n",
    "\n",
    "# Convert this to a DataFrame or a Series that can be added to nan_counts_per_user\n",
    "user_total_length_df = user_total_length.to_frame(name='total_length')\n",
    "\n",
    "# Merge this information with nan_counts_per_user\n",
    "# Since nan_counts_per_user already has user_id as its index, we can directly add the new column\n",
    "nan_counts_per_user['total_length'] = user_total_length_df['total_length']\n",
    "\n",
    "# Now, nan_counts_per_user includes the total_length column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the total NaN count for each feature across all users\n",
    "total_nan_counts = nan_counts_per_user.sum()\n",
    "\n",
    "# Assuming `X_with_users` is your original DataFrame and has the same number of entries for each user,\n",
    "# Calculate the total number of entries for a single feature across all users\n",
    "total_entries_per_feature = len(X_with_users)  # Or, more specifically, len(users) * average_entries_per_user if varies\n",
    "\n",
    "# Calculate the percentage of missing data for each feature\n",
    "percentage_missing = (total_nan_counts / total_entries_per_feature) * 100\n",
    "\n",
    "\n",
    "# Decide on a threshold for removing columns, e.g., 1%\n",
    "threshold = 0\n",
    "\n",
    "# Identify columns that exceed this threshold\n",
    "columns_to_remove = percentage_missing[percentage_missing > threshold].index.tolist()\n",
    "\n",
    "print(\"Removing \", len(columns_to_remove),\"columns out of \", len(X_with_users.columns))\n",
    "# Print out the columns to remove\n",
    "# print(\"Columns to remove due to excessive missing data:\", columns_to_remove)\n",
    "\n",
    "features_to_include = [feature for feature in features if feature not in columns_to_remove]\n",
    "\n",
    "if 'timestamp' in X_with_users.columns:\n",
    "    X_with_users['timestamp_numeric'] = X_with_users['timestamp'].astype(np.int64) // 10**9\n",
    "    # Ensure 'timestamp_numeric' is included and 'timestamp' is excluded from features_to_include\n",
    "    features_to_include = [f for f in features_to_include if f != 'timestamp'] + ['timestamp_numeric']\n",
    "\n",
    "# Continue with your existing preprocessing...\n",
    "user_df = X_with_users[X_with_users['user_id'] == users[-1]]\n",
    "median_values = user_df[features_to_include].median()\n",
    "user_df = user_df[features_to_include].fillna(median_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed a threshold check from 0 to 5 percent, testing each percentage, and then proceeded with increments of 5. Each test was conducted with a classifier and a linear model, where 0% consistently performed the best. We also evaluated several LSTM models and the standard techniques of forward fill, mean, median, and mode to address missing values. The median emerged as the second-best method, with the most effective approach being to ignore the values using a threshold of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Detail Check\n",
    "Testing to see if we need to do Binary Relevance, or Classifier chains. We can also check Label Powerset, but the issue is combinations can become very large since we have 52 labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv_data_4_model = combined_csv_data.copy()\n",
    "combined_csv_data_4_model['timestamp_numeric'] = combined_csv_data_4_model['timestamp'].astype(np.int64) // 10**9\n",
    "combined_csv_data_4_model = combined_csv_data_4_model.drop(columns=['timestamp'])\n",
    "X = combined_csv_data_4_model[features_to_include]\n",
    "y = combined_csv_data_4_model[output_columns]\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "corr_matrix = y.corr(method = 'pearson')  # Compute the correlation matrix\n",
    "\n",
    "# Flatten the matrix, sort by absolute value while preserving names\n",
    "corr_flat = corr_matrix.unstack()\n",
    "corr_flat_sorted = corr_flat.abs().sort_values(ascending=False)\n",
    "\n",
    "# Remove self-correlations\n",
    "corr_flat_sorted = corr_flat_sorted[corr_flat_sorted < 1]\n",
    "\n",
    "# Take the top N correlations for plotting (for simplicity, let's plot all unique pairs)\n",
    "unique_pairs = corr_flat_sorted.drop_duplicates().head(10)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "unique_pairs.plot(kind='bar')\n",
    "plt.title('Top Correlations Between Labels')\n",
    "plt.xlabel('Label Pairs')\n",
    "plt.ylabel('Correlation')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like there is a corrolation, lets do Classifier chains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "corr = y.corr(method = 'pearson')\n",
    "corr_flat = corr.unstack().sort_values(ascending =False)\n",
    "\n",
    "sns.heatmap(corr, annot = False, cmap = 'coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Classifier Chain with a RandomForest base classifier\n",
    "classifier = ClassifierChain(RandomForestClassifier())\n",
    "\n",
    "# Train the Classifier Chain model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Note: accuracy_score expects single-label predictions,\n",
    "# so for multi-label you might use another metric like hamming loss or a subset accuracy function\n",
    "# Here's an example with a custom subset accuracy for multi-label\n",
    "def subset_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).all(axis=1).mean()\n",
    "\n",
    "print(\"Subset Accuracy: \", subset_accuracy(y_test, predictions.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = users[0]\n",
    "\n",
    "models_data = {\n",
    "    'models': {},\n",
    "    'accuracies': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing to see if creating separate models is computationally expensive.\n",
    "\n",
    "#### Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv_data_4_model = combined_csv_data.copy()\n",
    "combined_csv_data_4_model['timestamp_numeric'] = combined_csv_data_4_model['timestamp'].astype(np.int64) // 10**9\n",
    "combined_csv_data_4_model = combined_csv_data_4_model.drop(columns=['timestamp'])\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for user in users:\n",
    "    \n",
    "    user_df = combined_csv_data_4_model[combined_csv_data['user_id'] == user]\n",
    "    \n",
    "    print(f'Shape of df of user no. {counter} of id {user} is: {user_df.shape}')\n",
    "    # Assuming 'combined_csv_data' is your DataFrame\n",
    "    X = user_df[features_to_include]\n",
    "    y = user_df[output_columns]\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize Classifier Chain with a RandomForest base classifier\n",
    "    classifier = ClassifierChain(RandomForestClassifier())\n",
    "    # Train the Classifier Chain model\n",
    "    classifier.fit(X_train, y_train)\n",
    "    models_data['models'][user] = classifier\n",
    "     # Make predictions\n",
    "    predictions = classifier.predict(X_test)\n",
    "    # Evaluate your model\n",
    "    # Example: Using accuracy score, you can choose other metrics as appropriate\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Note: accuracy_score expects single-label predictions,\n",
    "    # so for multi-label you might use another metric like hamming loss or a subset accuracy function\n",
    "    # Here's an example with a custom subset accuracy for multi-label\n",
    "    def subset_accuracy(y_true, y_pred):\n",
    "        return (y_true == y_pred).all(axis=1).mean()\n",
    "    accuracy = subset_accuracy(y_test, predictions.toarray())\n",
    "    models_data['accuracies'][user] = accuracy\n",
    "    print(f\"Subset Accuracy for {user}: \", accuracy)\n",
    "    with open('clfs_2.pkl', 'wb') as file:\n",
    "        pickle.dump(models_data, file)\n",
    "    print('File Updated')\n",
    "    counter = counter +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of df is: {combined_csv_data_4_model.shape}')\n",
    "# Assuming 'combined_csv_data' is your DataFrame\n",
    "X = combined_csv_data_4_model[features_to_include]\n",
    "y = combined_csv_data_4_model[output_columns]\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Classifier Chain with a RandomForest base classifier\n",
    "classifier = ClassifierChain(RandomForestClassifier())\n",
    "# Train the Classifier Chain model\n",
    "classifier.fit(X_train, y_train)\n",
    "models_data['models'][\"all\"] = classifier\n",
    "    # Make predictions\n",
    "print('Processing predictions for X_test.')\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "def subset_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).all(axis=1).mean()\n",
    "print('Processing accuracy.')\n",
    "\n",
    "accuracy = subset_accuracy(y_test, predictions.toarray())\n",
    "models_data['accuracies'][\"all\"] = accuracy\n",
    "print(f\"Subset Accuracy for all together data is: \", accuracy)\n",
    "with open('all_clfs_2.pkl', 'wb') as file:\n",
    "    pickle.dump(models_data, file)\n",
    "print('File Updated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the IoT, we will use the general model and use the individual model once the accuracy or individual model passes the general model. Both of these iterations are to predict the current labels using sensor data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using classifier chains to generate predictions for LSTM\n",
    "Let's use the success of our classifier chains for individual users to generate predictions for our LSTM model. First we'll prepare the predictions generated by the classifier chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing dataset for predictions\n",
    "combined_csv_data_4_model = combined_csv_data.copy()\n",
    "combined_csv_data_4_model['timestamp_numeric'] = pd.to_datetime(combined_csv_data_4_model['timestamp']).astype(np.int64) // 10**9\n",
    "combined_csv_data_4_model = combined_csv_data_4_model.drop(columns=['timestamp'])\n",
    "\n",
    "# Creating user_specific_data dictionary\n",
    "user_specific_data = {}\n",
    "for user in users:\n",
    "    user_df = combined_csv_data_4_model[combined_csv_data_4_model['user_id'] == user]\n",
    "    \n",
    "    # Sorting user_df by 'timestamp_numeric' to ensure temporal order\n",
    "    user_df = user_df.sort_values(by='timestamp_numeric')\n",
    "    user_specific_data[user] = user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading models from disk\n",
    "with open('clfs_2.pkl', 'rb') as file:\n",
    "    models_data = pickle.load(file)\n",
    "\n",
    "# Defining function to generate predictions using classifier chains\n",
    "def generate_classifier_chain_predictions(user_df, classifier_chain_model):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(user_df[features_to_include])\n",
    "\n",
    "    # Generating predictions\n",
    "    predictions = classifier_chain_model.predict(X_scaled)\n",
    "    return predictions.toarray()\n",
    "\n",
    "user_predictions = {}\n",
    "for user_id, user_data in user_specific_data.items():\n",
    "    user_df = user_specific_data[user_id]\n",
    "    classifier_chain_model = models_data['models'][user_id]\n",
    "    user_predictions[user_id] = generate_classifier_chain_predictions(user_df, classifier_chain_model)\n",
    "\n",
    "# Now 'user_predictions' contains predictions for each user that can be used as input for the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking if all predictions are 2D arrays with a consistent second dimension\n",
    "for user_id, predictions in user_predictions.items():\n",
    "    print(f\"User ID: {user_id}, Shape: {np.array(predictions).shape}\")\n",
    "\n",
    "consistent_shape = True\n",
    "second_dim = None\n",
    "\n",
    "for predictions in user_predictions.values():\n",
    "    np_predictions = np.array(predictions)\n",
    "    if second_dim is None:\n",
    "        second_dim = np_predictions.shape[1] if len(np_predictions.shape) > 1 else 0\n",
    "    elif len(np_predictions.shape) <= 1 or np_predictions.shape[1] != second_dim:\n",
    "        consistent_shape = False\n",
    "        break\n",
    "\n",
    "if consistent_shape and second_dim:\n",
    "    print(f\"All predictions are 2D arrays with a consistent second dimension: {second_dim}\")\n",
    "else:\n",
    "    print(\"Predictions are not consistent 2D arrays or have varying second dimensions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding and stacking for LSTM input shpae\n",
    "max_sequence_length = max([len(predictions) for predictions in user_predictions.values()])\n",
    "X_lstm = pad_sequences(list(user_predictions.values()), maxlen=max_sequence_length, padding='post', dtype='float64')\n",
    "original_lengths = [len(predictions) for predictions in user_predictions.values()]\n",
    "X_lstm = X_lstm.reshape((X_lstm.shape[0], X_lstm.shape[1], 52))\n",
    "\n",
    "print(f\"LSTM input shape: {X_lstm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating user specific timestamp dictionaries\n",
    "user_specific_timestamps = {}\n",
    "for user in users:\n",
    "    user_df = combined_csv_data[combined_csv_data['user_id'] == user]\n",
    "    user_df = user_df.sort_values(by='timestamp')\n",
    "    user_specific_timestamps[user] = user_df['timestamp'].values\n",
    "\n",
    "# Padding the timestamps to have the same length as the sequences\n",
    "padded_timestamps = pad_sequences(list(user_specific_timestamps.values()), maxlen=max_sequence_length, padding='post', value=None, dtype='float64')  # Use \"NONE\" as a placeholder for non-real timestamps\n",
    "original_timestamp_lengths = [len(ts) for ts in user_specific_timestamps.values()]\n",
    "\n",
    "# Now 'padded_timestamps' contains the ordered timestamps for each user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(X_lstm.shape[1], X_lstm.shape[2]), return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(len(label_columns), activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting user labels\n",
    "user_labels = {}\n",
    "for user_id in users:\n",
    "    user_labels_df = combined_csv_data[combined_csv_data['user_id'] == user_id]\n",
    "    labels_array = user_labels_df[label_columns].values \n",
    "    user_labels[user_id] = labels_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shape for padding\n",
    "for user_id, labels in user_labels.items():\n",
    "    print(f\"User ID: {user_id}, Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding labels\n",
    "padded_labels = []\n",
    "\n",
    "for user_id, labels in user_labels.items():\n",
    "    # Padding the user's label array to have the same length as the max_sequence_length\n",
    "    padded_label = pad_sequences([labels], maxlen=max_sequence_length, padding='post', dtype='float64')[0]\n",
    "    padded_labels.append(padded_label)\n",
    "\n",
    "y_lstm = np.array(padded_labels)\n",
    "\n",
    "print(f\"Padded labels shape: {y_lstm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll reduce the dimensionality of y_lstm to just two dimensions: (number of samples, number of labels)\n",
    "y_lstm = y_lstm[:, 0, :] \n",
    "\n",
    "print(f\"Adjusted labels shape: {y_lstm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data for training & validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluating model\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "print(f'Validation accuracy: {val_acc}, Validation loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions for the validation set\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "# Applying threshold to convert propabilities to binary values\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Lets examine a few predictions to get a sense of what our model is doing \n",
    "for i, prediction in enumerate(binary_predictions[:5]):\n",
    "    print(f\"Prediction for sample {i}: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets map these to the label names so we know what the predictions mean \n",
    "for i, prediction in enumerate(binary_predictions[:5]):\n",
    "    labeled_prediction = dict(zip(label_columns, prediction))\n",
    "    print(f\"Prediction for sample {i}: {labeled_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all predictions into a dataframe\n",
    "predictions_df = pd.DataFrame(binary_predictions, columns=label_columns)\n",
    "predictions_df.to_csv('LSTM_model_predictions_with_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding linear model for input predictions\n",
    "Let's see if we can leverage the predicted labels from our LSTM model to use a linear model for input predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "X = y_lstm \n",
    "y = X_lstm.reshape(X_lstm.shape[0], -1) \n",
    "\n",
    "# Splitting the dataset for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions - predicting features based on labels\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "# Evaluating model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving predictions to csv\n",
    "predictions_df = pd.DataFrame(y_pred, columns=[f'Feature_{i}' for i in range(y_pred.shape[1])])\n",
    "predictions_df.to_csv('predicted_features.csv', index=False)\n",
    "print(\"Predictions saved to 'predicted_features.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old numbers\n",
    "Shape of df is: (377346, 280)\n",
    "Processing predictions for X_test.\n",
    "Processing accuracy.\n",
    "Subset Accuracy for all together data is:  0.40275606201139524\n",
    "File Updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentations\n",
    "\n",
    "This section includes all the code tested during the model creation. Different configurations along with different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP model\n",
    "class MultiTaskMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MultiTaskMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.output_layer = nn.Linear(64, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Placeholder for models and accuracies\n",
    "models_data_2 = {\n",
    "    'models': {},\n",
    "    'accuracies': {}\n",
    "}\n",
    "\n",
    "# Loop through each user\n",
    "for user in users:\n",
    "    user_df = combined_csv_data_4_model[combined_csv_data_4_model['user_id'] == user]\n",
    "    X = user_df[features_to_include].values\n",
    "    y = user_df[output_columns].values\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test)\n",
    "    \n",
    "    # DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MultiTaskMLP(input_size=X_train_tensor.shape[1], output_size=y_train_tensor.shape[1])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(10):  # Adjust epochs as needed\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = torch.sigmoid(outputs) > 0.5  # Threshold at 0.5\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).float().mean()\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # Store the model and accuracy\n",
    "    models_data_2['models'][user] = model.state_dict()  # Store state dict for minimal size\n",
    "    models_data_2['accuracies'][user] = accuracy.item()\n",
    "    print(f\"User {user}: Accuracy = {accuracy.item():.4f}\")\n",
    "\n",
    "# Save the models and accuracies\n",
    "with open('mlp_models.pkl', 'wb') as file:\n",
    "    pickle.dump(models_data_2, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(combined_csv_data_4_model[features_to_include])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, combined_csv_data_4_model[output_columns], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Convert dataset to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values)  # For multi-label\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values)  # For multi-label\n",
    "\n",
    "\n",
    "# DataLoader setup\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Model setup\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(len(features_to_include), 64),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, len(output_columns))\n",
    ")\n",
    "\n",
    "# Loss and optimizer setup\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    for inputs, labels in train_loader:\n",
    "        lr = 0.0001 * (epoch + 1)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/10, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.sigmoid(outputs) > 0.5  # Applying sigmoid and threshold for multi-label\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.byte()).all(dim=1).sum().item()  # Adjust for multi-label accuracy\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = MinMaxScaler(feature_range=(-1,1))\n",
    "user_df_scaled = s1.fit_transform(user_df)\n",
    "user_df_scaled = pd.DataFrame(user_df_scaled, columns=features_to_include)\n",
    "\n",
    "# Create sequences\n",
    "look_back = 4\n",
    "generator = TimeseriesGenerator(user_df_scaled.values, user_df_scaled.values,\n",
    "                                length=look_back, batch_size=1)\n",
    "def create_lstm_model(input_shape, num_features):\n",
    "    model = Sequential([\n",
    "        LSTM(units = num_features, activation='relu', input_shape=input_shape, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_features),\n",
    "        LSTM(units=num_features, return_sequences=True),\n",
    "        Dropout(0.2),        \n",
    "        LSTM(units=num_features, return_sequences=True),\n",
    "        Dense(num_features),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units=num_features, return_sequences=True),\n",
    "        Dense(num_features),\n",
    "        Dropout(0.2),        \n",
    "        LSTM(units=num_features),\n",
    "        Dense(num_features),\n",
    "        Activation('linear'),\n",
    "        \n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics = ['accuracy'])  \n",
    "    return model\n",
    "model_path = 'LSTM_next_model.h5'\n",
    "# Define and compile the LSTM model\n",
    "model = create_lstm_model((look_back, len(features_to_include)), len(features_to_include))\n",
    "\n",
    "import keras\n",
    "if os.path.exists(model_path):\n",
    "    load_model(model_path)\n",
    "else:\n",
    "\n",
    "    model.fit(generator, epochs=1) \n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_df(df, model, features_to_include, look_back=4):\n",
    "    df_filtered = df[features_to_include]\n",
    "    \n",
    "    if len(df_filtered) >= look_back:\n",
    "        # Extract the last `look_back` rows for the prediction\n",
    "        last_sequences = df_filtered[-look_back:].values.reshape((1, look_back, len(features_to_include)))\n",
    "    else:\n",
    "        raise ValueError(f\"DataFrame must have at least {look_back} rows for prediction.\")\n",
    "    \n",
    "    # Predict the next row using the LSTM model\n",
    "    predictions = model.predict(last_sequences)\n",
    "    \n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def compare_predictions_with_actual(df, model, features_to_include, look_back=3):\n",
    "    predictions = predict_from_df(user_df.iloc[1:5], model, features_to_include, look_back)\n",
    "    predictions = s1.inverse_transform(predictions)\n",
    "    actual_values = user_df.iloc[4][features_to_include].values  # Adjust index if needed\n",
    "    \n",
    "    return predictions, actual_values\n",
    "\n",
    "# Assuming 'user_df' is already preprocessed appropriately, including scaling\n",
    "predictions, actual_values = compare_predictions_with_actual(user_df, model, features_to_include, look_back=4)\n",
    "\n",
    "# Now you can compare 'predictions' with 'actual_values'\n",
    "# Note: If your data was scaled, you might need to inverse scale both predictions and actual values before comparison\n",
    "\n",
    "print(f\"{'Predictions':<15}   | {'Actual Values':<15} | {'Diff':<15} | {'Diff %':<15}\")\n",
    "print(\"-\" * 47)  # Adjust the number based on the width of your columns\n",
    "\n",
    "j = 0\n",
    "for i in range(len(predictions[j])):\n",
    "    p = predictions[j][i]\n",
    "    a = actual_values[i]\n",
    "    diff = a-p\n",
    "    diff_p = ((p-a)/a)*100\n",
    "    print(f\"{p:.6f}{'':<9} | {a:<15} | {diff:.6f} | {diff_p:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing CNN And LSTM for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adjust the input shape according to your dataset\n",
    "input_shape = (X_train.shape[1], 1)  # Assuming non-sequential data for simplicity\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    # Instead of Flatten, use Reshape or adjust the model so it's suitable for LSTM input\n",
    "    # Reshape example (adjust the target shape according to your needs):\n",
    "    # This line is illustrative; actual reshaping depends on the output shape of the previous layer\n",
    "    Reshape((-1, 128)),  # Adjust the target shape\n",
    "    LSTM(50, return_sequences=False),  # If you want the LSTM to output a sequence, set return_sequences=True\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(len(label_columns), activation='sigmoid')  # Use 'sigmoid' for multi-label classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',  # Use 'binary_crossentropy' for multi-label classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for CNN if needed\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For arrays, assuming X_train_reshaped and y_train are your features and labels respectively\n",
    "subset_size = 100  # Choose a small size for quick tests\n",
    "X_train_subset = X_train_reshaped[:subset_size]\n",
    "y_train_subset = y_train[:subset_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Shape Issues\n",
    "model_name = 'first_try.h5'\n",
    "if os.path.exists(model_name):\n",
    "    model = load_model(model_name)\n",
    "else:\n",
    "    history = model.fit(X_train, y_train,\n",
    "                    epochs=1,  \n",
    "                    batch_size=64, \n",
    "                    validation_split=0.2, \n",
    "                    verbose=1)  \n",
    "\n",
    "    model.save(model_name)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test_reshaped, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * math.exp(-0.1)\n",
    "\n",
    "callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Reshape((-1, 128)),  # Adjust based on the output shape of the previous layer\n",
    "    Bidirectional(LSTM(100, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dense(len(label_columns), activation='sigmoid')  # Adjust based on your label columns\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model_name = 'second_try.h5'\n",
    "if os.path.exists(model_name):\n",
    "    model = load_model(model_name)\n",
    "else:\n",
    "    model.fit(X_train, y_train, epochs=1,batch_size=128,  validation_split=0.2)\n",
    "    model.save(model_name)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test_reshaped, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example fitting with callbacks\n",
    "# model.fit(X_train, y_train, epochs=1,batch_size=128,  validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * math.exp(-0.1)\n",
    "\n",
    "callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Reshape((-1, 128)),  # Adjust based on the output shape of the previous layer\n",
    "    Bidirectional(LSTM(100, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dense(len(label_columns), activation='sigmoid')  # Adjust based on your label columns\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'third_try.h5'\n",
    "if os.path.exists(model_name):\n",
    "    model = load_model(model_name)\n",
    "else:\n",
    "    model.fit(X_train, y_train, epochs=1,batch_size=128,  validation_split=0.2)\n",
    "    model.save(model_name)\n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(X_test_reshaped, y_test, verbose=2)\n",
    "    print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test_reshaped, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'third_try_2.h5'\n",
    "if os.path.exists(model_name):\n",
    "    model = load_model(model_name)\n",
    "else:\n",
    "    model.fit(X_train, y_train, epochs=1,batch_size=20,  validation_split=0.2)\n",
    "    model.save(model_name)\n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(X_test_reshaped, y_test, verbose=2)\n",
    "    print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Adjust the input shape according to your dataset\n",
    "input_shape = (X_train.shape[1], 1)  # Assuming non-sequential data for simplicity\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(len(label_columns), return_sequences=True, input_shape=input_shape),  # If you want the LSTM to output a sequence, set return_sequences=True\n",
    "    Dropout(0.2),\n",
    "    Conv1D(filters=len(label_columns), kernel_size=2, activation='relu'),\n",
    "    MaxPooling1D(pool_size=7),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    # Instead of Flatten, use Reshape or adjust the model so it's suitable for LSTM input\n",
    "    # Reshape example (adjust the target shape according to your needs):\n",
    "    # This line is illustrative; actual reshaping depends on the output shape of the previous layer\n",
    "    Reshape((-1, 128)),  # Adjust the target shape\n",
    "    LSTM(50, return_sequences=False),  # If you want the LSTM to output a sequence, set return_sequences=True\n",
    "    Dropout(0.2),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(len(label_columns), activation='sigmoid')  # Use 'sigmoid' for multi-label classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',  # Use 'binary_crossentropy' for multi-label classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'fourth_try_2.h5'\n",
    "if os.path.exists(model_name):\n",
    "    model = load_model(model_name)\n",
    "else:\n",
    "    model.fit(X_train, y_train, epochs=1, validation_split=0.2)\n",
    "    model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Adjust the input shape according to your dataset\n",
    "# For LSTM, input should be in the form of (samples, timesteps, features)\n",
    "# Assuming each sample is a sequence of vectors\n",
    "input_shape = (X_train.shape[1], 1)  # Adjust '1' if your data is already in sequences\n",
    "\n",
    "model = Sequential()\n",
    "# Start with an LSTM layer to process sequences\n",
    "model.add(LSTM(units=64, return_sequences=True, input_shape=input_shape))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Followed by CNN layers for feature extraction from sequences processed by LSTM\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten the output to feed into a dense layer\n",
    "model.add(Flatten())\n",
    "# Additional dense layers or LSTM layers can be added here if needed\n",
    "# Example: model.add(LSTM(50, return_sequences=False))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))  # Assuming 'y' is one-hot encoded for multi-label classification\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',  # Adjust the loss function as per your problem\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'fifth_try_3.h5'\n",
    "if os.path.exists(model_name):\n",
    "    model = load_model(model_name)\n",
    "else:\n",
    "    model.fit(X_train, y_train, epochs=1, validation_split=0.2)\n",
    "    model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'X' and 'y' are your features and labels, respectively\n",
    "\n",
    "# Data Preprocessing\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Normalize features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Definition\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(64, kernel_size=2, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(y_train.shape[1], activation='sigmoid')  # Output layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])  # Add other metrics as needed\n",
    "\n",
    "# Custom Callback for Precision, Recall, F1 Score\n",
    "class MetricsCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_predict = (np.asarray(self.model.predict(X_test))).round()\n",
    "        val_targ = y_test\n",
    "        _val_precision = precision_score(val_targ, val_predict, average='micro')\n",
    "        _val_recall = recall_score(val_targ, val_predict, average='micro')\n",
    "        _val_f1 = f1_score(val_targ, val_predict, average='micro')\n",
    "        print(f' — val_precision: {_val_precision:.4f} — val_recall: {_val_recall:.4f} — val_f1: {_val_f1:.4f}')\n",
    "\n",
    "# Model Training\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=1,  # Adjust number of epochs as necessary\n",
    "          batch_size=32,  # Adjust batch size as necessary\n",
    "          callbacks=[MetricsCallback()])\n",
    "\n",
    "# Note: This is a simplified example. In practice, you might need to adjust the model architecture, preprocessing steps,\n",
    "# and training parameters based on the specifics of your dataset and task.\n",
    "\n",
    "model_name = 'fifth_try_4.h5'\n",
    "if os.path.exists(model_name):\n",
    "    model = load_model(model_name)\n",
    "else:\n",
    "    model.fit(X_train, y_train, validation_split=0.2, epochs=1, batch_size=32, callbacks=[MetricsCallback()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy = build_hierarchy(X.columns)\n",
    "formatted_hierarchy = format_hierarchy(hierarchy)\n",
    "print(formatted_hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Batch Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_best_batch_size(model, X_train, y_train, X_test, y_test, batch_sizes):\n",
    "    \"\"\"\n",
    "    Trains a given model using different batch sizes, evaluates performance on test data,\n",
    "    stores each trained model, and returns the best batch size along with its accuracy and a dictionary of models.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The initial model to be trained.\n",
    "    - X_train, y_train: Training data and labels.\n",
    "    - X_test, y_test: Test data and labels.\n",
    "    - batch_sizes: List of batch sizes to test.\n",
    "\n",
    "    Returns:\n",
    "    - best_batch_size: The batch size yielding the highest accuracy on test data.\n",
    "    - best_acc: The highest accuracy achieved on test data.\n",
    "    - models_dict: A dictionary of saved model filenames keyed by their batch sizes.\n",
    "    \"\"\"\n",
    "    models_dict = {}\n",
    "    best_acc = 0\n",
    "    best_batch_size = None\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Training with batch size: {batch_size}\")\n",
    "        # Clone the original model architecture for a fair comparison\n",
    "        model_clone = clone_model(model)\n",
    "        model_clone.compile(optimizer=model.optimizer, loss=model.loss, metrics=model.metrics)\n",
    "        \n",
    "        # Fit the model\n",
    "        model_clone.fit(X_train, y_train,\n",
    "                        epochs=2, \n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=1)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        test_loss, test_acc = model_clone.evaluate(X_test, y_test, verbose=2)\n",
    "        print(f\"Test accuracy: {test_acc}, Test loss: {test_loss}\")\n",
    "        \n",
    "        # Save the model\n",
    "        model_file_name = f'ExtraSensory_CNN_LSTM_bs{batch_size}.h5'\n",
    "        model_clone.save(model_file_name)\n",
    "        models_dict[batch_size] = model_file_name\n",
    "        \n",
    "        # Update best model if current is better\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_batch_size = batch_size\n",
    "\n",
    "    print(f\"Best Batch Size: {best_batch_size} with Test Accuracy: {best_acc}\")\n",
    "    return best_batch_size, best_acc, models_dict\n",
    "\n",
    "# Example usage:\n",
    "batch_sizes = [128, 64, 16, 4, 1, None] \n",
    "# Call the function and store its return values\n",
    "best_batch_size, best_acc, models_dict = find_best_batch_size(model, X_train_reshaped, y_train, X_test_reshaped, y_test, batch_sizes)\n",
    "\n",
    "# Now `models_dict` is available outside of the function\n",
    "print(\"Available models and their batch sizes:\")\n",
    "for batch_size, model_path in models_dict.items():\n",
    "    print(f\"Batch Size: {batch_size}, Model Path: {model_path}\")\n",
    "\n",
    "# You can load any model from `models_dict` for further use\n",
    "# selected_model_path = models_dict[best_batch_size]\n",
    "# loaded_model = load_model(selected_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test_reshaped, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ExtraSensory_CNN_LSTM_Model_v2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train,\n",
    "                    epochs=10, \n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train,\n",
    "                    epochs=2, \n",
    "                    batch_size=20,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test_reshaped, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ExtraSensory_CNN_LSTM_Model_v2_bs_20.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train,\n",
    "                    epochs=2, \n",
    "                    batch_size=2,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test_reshaped, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ExtraSensory_CNN_LSTM_Model_v2_bs_2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train,\n",
    "                    epochs=2, \n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test_reshaped, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')\n",
    "model.save('ExtraSensory_CNN_LSTM_Model_v2_bs_1.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the total NaN count for each feature across all users\n",
    "total_nan_counts = nan_counts_per_user.sum()\n",
    "\n",
    "# Assuming `X_with_users` is your original DataFrame and has the same number of entries for each user,\n",
    "# Calculate the total number of entries for a single feature across all users\n",
    "total_entries_per_feature = len(X_with_users)  # Or, more specifically, len(users) * average_entries_per_user if varies\n",
    "\n",
    "# Calculate the percentage of missing data for each feature\n",
    "percentage_missing = (total_nan_counts / total_entries_per_feature) * 100\n",
    "\n",
    "\n",
    "def testing_threshold(threshold, testing_user, fill_type= 'ffill', epochs = 1 ):\n",
    "\n",
    "    # Decide on a threshold for removing columns, e.g., 1%\n",
    "    threshold = threshold\n",
    "\n",
    "    # Identify columns that exceed this threshold\n",
    "    columns_to_remove = percentage_missing[percentage_missing > threshold].index.tolist()\n",
    "\n",
    "    print(\"Removing \", len(columns_to_remove),\"columns out of \", len(X_with_users.columns))\n",
    "    # Print out the columns to remove\n",
    "    # print(\"Columns to remove due to excessive missing data:\", columns_to_remove)\n",
    "\n",
    "    features_to_include = [feature for feature in features if feature not in columns_to_remove]\n",
    "\n",
    "    # First User \n",
    "    user_df = X_with_users[X_with_users['user_id'] == users[testing_user]]\n",
    "    \n",
    "    if fill_type == 'ffill':\n",
    "    # Forward fill\n",
    "        user_df = user_df[features_to_include].ffill()\n",
    "    elif fill_type == 'mean':\n",
    "        # Fill missing values with the mean of each column\n",
    "        mean_values = user_df[features_to_include].mean()\n",
    "        user_df = user_df[features_to_include].fillna(mean_values)\n",
    "    elif fill_type == 'median':\n",
    "        # Fill missing values with the median of each column\n",
    "        median_values = user_df[features_to_include].median()\n",
    "        user_df = user_df[features_to_include].fillna(median_values)\n",
    "    elif fill_type == 'zero':\n",
    "        # Fill missing values with zero\n",
    "        user_df = user_df[features_to_include].fillna(0)\n",
    "    else:\n",
    "        # If no valid fill_type is provided, print a warning or fill with a default method\n",
    "        print(\"Invalid fill_type. No changes made to user_df.\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    user_df[features_to_include] = scaler.fit_transform(user_df)\n",
    "\n",
    "    # Define LSTM model architecture\n",
    "    def create_lstm_model(input_shape):\n",
    "        model = Sequential([\n",
    "            LSTM(50, activation='relu', input_shape=input_shape),\n",
    "            Dense(len(features_to_include), activation='relu') \n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001, clipvalue=0.5), loss='mse')\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    # Assuming timestamps are sorted; if not, sort user_df by timestamp here\n",
    "    user_df.sort_values('timestamp', inplace=True)\n",
    "\n",
    "    # Convert user_df to sequences for LSTM\n",
    "    look_back = 3 \n",
    "    generator = TimeseriesGenerator(user_df[features_to_include].values, user_df[features_to_include].values,\n",
    "                                    length=look_back, batch_size=1)\n",
    "\n",
    "    # Create and train LSTM model on the selected user's data\n",
    "    model = create_lstm_model((look_back, len(features_to_include)))\n",
    "    model.fit(generator, epochs=epochs, verbose=1)  # Adjust epochs and verbosity as needed\n",
    "\n",
    "    return model, features_to_include\n",
    "\n",
    "\n",
    "threshold_testing_models = {}\n",
    "models_with_threshhold = {}\n",
    "features_to_include = {}\n",
    "\n",
    "for i in [0,1,2,3,4,5,10,15,20,25,30,40,50]:\n",
    "    print(\"for threshold: \", i)\n",
    "    models_with_threshhold[i], features_to_include[i] = testing_threshold(i, -1)\n",
    "\n",
    "\n",
    "fill_options = ['mean', 'median', 'zero']\n",
    "for fill in fill_options:\n",
    "    print(\"Checking different fills at 5 threshhold with \", fill, ' fill option')\n",
    "    models_with_threshhold[fill], features_to_include[fill] = testing_threshold(5, -1, fill_type = fill)\n",
    "\n",
    "\n",
    "\n",
    "# The best one came out to be threshold = 0 fill =median with epoch of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing eopchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"threshold0_median_epoch3\"\n",
    "models_with_threshhold[model_name], features_to_include[model_name]  = testing_threshold(0, -1, fill_type = 'median', epochs= 3)\n",
    "\n",
    "model_name = \"threshold0_median_epoch2\"\n",
    "models_with_threshhold[model_name], features_to_include[model_name]  = testing_threshold(0, -1, fill_type = 'median', epochs= 2)\n",
    "\n",
    "model_name = \"threshold0_median_epoch1\"\n",
    "models_with_threshhold[model_name], features_to_include[model_name]  = testing_threshold(0, -1, fill_type = 'median')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best one came up to be: threshold0_median_epoch1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_one_so_far = 'threshold0_median_epoch1'\n",
    "print(features_to_include[best_model_one_so_far])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_df(df, model, features_to_include, look_back=3):\n",
    "    \"\"\"\n",
    "    Process the given DataFrame and predict the next value using the LSTM model.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to process and predict from.\n",
    "    - model: Trained LSTM model to use for predictions.\n",
    "    - features_to_include: List of feature names to include in the prediction.\n",
    "    - look_back: Number of previous time steps to use as input for predictions.\n",
    "    \n",
    "    Returns:\n",
    "    - predictions: Predicted values for the next time step.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame contains the necessary features\n",
    "    if not all(feature in df.columns for feature in features_to_include):\n",
    "        raise ValueError(\"DataFrame missing required features\")\n",
    "    \n",
    "    # Fill missing values with median\n",
    "    df_filled = df[features_to_include].fillna(df[features_to_include].median())\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler().fit(df_filled)\n",
    "    df_scaled = scaler.transform(df_filled)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = np.array([df_scaled[i - look_back:i] for i in range(look_back, len(df_scaled) + 1)])\n",
    "    \n",
    "    # Predict using the LSTM model\n",
    "    predictions = model.predict(sequences)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "models , features_for_model = {} , {}\n",
    "# Example of how to use the function\n",
    "# Ensure 'model', 'features_to_include', and 'look_back' are defined as per your model's training setup\n",
    "predictions = predict_from_df(df_user.iloc[1:4],  models_with_threshhold[best_model_one_so_far], features_to_include[best_model_one_so_far], look_back=3)\n",
    "print(predictions)\n",
    "print(features_for_model[model_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will go with the lowest threshold and with median fill option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total NaN count for each feature across all users\n",
    "total_nan_counts = nan_counts_per_user.sum()\n",
    "\n",
    "# Assuming `X_with_users` is your original DataFrame and has the same number of entries for each user,\n",
    "# Calculate the total number of entries for a single feature across all users\n",
    "total_entries_per_feature = len(X_with_users)  # Or, more specifically, len(users) * average_entries_per_user if varies\n",
    "\n",
    "# Calculate the percentage of missing data for each feature\n",
    "percentage_missing = (total_nan_counts / total_entries_per_feature) * 100\n",
    "\n",
    "# Decide on a threshold for removing columns, e.g., 1%\n",
    "threshold = 0\n",
    "\n",
    "# Identify columns that exceed this threshold\n",
    "columns_to_remove = percentage_missing[percentage_missing > threshold].index.tolist()\n",
    "\n",
    "print(len(columns_to_remove),\"out of \", len(X_with_users.columns))\n",
    "# Print out the columns to remove\n",
    "print(\"Columns to remove due to excessive missing data:\", columns_to_remove)\n",
    "\n",
    "features = [feature for feature in features if feature not in columns_to_remove]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = df_user[features].isna().sum()\n",
    "nan_count_sorted = nan_count.sort_values(ascending=False)\n",
    "print(len(nan_count_sorted))\n",
    "nan_count_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more features as necessary\n",
    "df_user = df_user[features].fillna(method='ffill')\n",
    "scaler = StandardScaler()\n",
    "df_user[features] = scaler.fit_transform(df_user)\n",
    "\n",
    "# Define LSTM model architecture\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=input_shape),\n",
    "        Dense(1) \n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001, clipvalue=0.5), loss='mse')  # Apply gradient clipping\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Assuming timestamps are sorted; if not, sort user_data by timestamp here\n",
    "df_user.sort_values('timestamp', inplace=True)\n",
    "\n",
    "# Convert user_data to sequences for LSTM\n",
    "look_back = 5  \n",
    "generator = TimeseriesGenerator(df_user[features].values, df_user[features].values,\n",
    "                                length=look_back, batch_size=1)\n",
    "\n",
    "# Create and train LSTM model on the selected user's data\n",
    "model = create_lstm_model((look_back, len(features)))\n",
    "model.fit(generator, epochs=2, verbose=1)  # Adjust epochs and verbosity as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = X_with_users[X_with_users['user_id'] == users[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and fill missing values function\n",
    "def predict_and_fill_missing_values(data, model, feature_columns, look_back):\n",
    "    for i in range(len(data)):\n",
    "        if pd.isnull(df.loc[i, feature_columns]).any():  # Check if any feature value is missing\n",
    "            input_seq = data[feature_columns].iloc[max(i-look_back, 0):i].values\n",
    "            input_seq = scaler.transform(input_seq)  # Normalize the input\n",
    "            input_seq = input_seq.reshape((1, look_back, len(feature_columns)))\n",
    "            predicted_value = model.predict(input_seq)\n",
    "            data.loc[i, feature_columns] = scaler.inverse_transform(predicted_value)  # Fill with prediction\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Fill missing values in the original DataFrame\n",
    "df_filled_with_predictions = predict_and_fill_missing_values(df_test, model, features, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in features:\n",
    "    # Find indices with missing values for the current column\n",
    "    missing_indices = df_test[df_test[column].isnull()].index.tolist()\n",
    "    \n",
    "    for missing_index in missing_indices:\n",
    "        # Check if there are enough previous data points\n",
    "        if missing_index >= look_back:\n",
    "            # Prepare the input sequence for prediction\n",
    "            # Assuming all features in 'features' list are used for prediction\n",
    "            input_sequence = df_test[features].iloc[missing_index-look_back:missing_index].values\n",
    "            input_sequence = scaler.transform(input_sequence)  # Scale the sequence according to previous scaler fit\n",
    "            input_sequence = input_sequence.reshape((1, look_back, len(features)))\n",
    "            \n",
    "            # Predict the missing value\n",
    "            predicted_value = model.predict(input_sequence)\n",
    "            predicted_value = scaler.inverse_transform(predicted_value)  # Assuming the model predicts the target column\n",
    "            \n",
    "            # Update the DataFrame with the predicted value\n",
    "            # Here, we handle single-feature prediction; if predicting multiple, adjust accordingly\n",
    "            df_test.at[missing_index, column] = predicted_value[0, 0]  # Adjust indexing based on your prediction shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = combined_csv_data[output_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_check(df, df_name = \"BrokenPipeError\"):\n",
    "    missing_values = df.isna().sum()\n",
    "    missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "    if len(missing_values) > 0:\n",
    "        plt.figure(figsize=(15, 60))\n",
    "        missing_values.sort_values(ascending=True).plot(kind='barh')\n",
    "        plt.title(f'Missing Values in Each Column ({df_name})')\n",
    "        plt.xlabel('Columns')\n",
    "        plt.ylabel('Number of Missing Values')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print('All the missing values have been covered.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_check(y, 'y')\n",
    "y.fillna(0, inplace=True)\n",
    "missing_value_check(y, 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(len(input_columns),)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(output_columns))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model to a file\n",
    "model.save(\"tf_model_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[1:3])\n",
    "y_pred_0 = model.predict(X_test[1:3])\n",
    "print(y_pred_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting loss and validation loss values\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "# Creating epoch numbers (starting from 1)\n",
    "epochs_range = range(1, epochs + 1)\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epochs_range, training_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs_range, validation_loss, 'ro-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before splitting, ensure there are no NaN values in your output columns\n",
    "for col in output_columns:\n",
    "    combined_csv_data[col].fillna(0, inplace=True)  # Replace NaN in y with 0, if appropriate\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_csv_data[input_columns], combined_csv_data[output_columns], test_size=0.2, random_state=42)\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for output_col in output_columns:\n",
    "    # Create a pipeline with an imputer (to fill missing values in features) and logistic regression\n",
    "    pipeline = make_pipeline(\n",
    "        SimpleImputer(strategy='mean'),  # Fills missing X values with the mean of each column\n",
    "        LogisticRegression(max_iter=1000)  # Increased max_iter to ensure convergence\n",
    "    )\n",
    "    \n",
    "    # Fit the pipeline to the training data\n",
    "    pipeline.fit(X_train, y_train[output_col])\n",
    "    \n",
    "    # New data for prediction. This example is simplified and should be replaced with actual new data.\n",
    "    # Ensure X_new has the same number of features as X_train. Here, we use np.nan as placeholders.\n",
    "    X_new = np.array([[0.5, 1.2] + [np.nan] * (224)])  # Adjusted to match the feature count of the trained model\n",
    "    \n",
    "    # Predicting the probability for the given X_new\n",
    "    pred_prob = pipeline.predict_proba(X_new)[0][1]\n",
    "    \n",
    "    # Storing the prediction\n",
    "    predictions[output_col] = pred_prob\n",
    "\n",
    "# Displaying the predicted probabilities\n",
    "for y_col, prob in predictions.items():\n",
    "    print(f\"Predicted probability for {y_col}: {prob:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names, but SimpleImputer was fitted with feature names\")\n",
    "\n",
    "# Fill missing values in output columns with a default value (e.g., 0)\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dictionary to store the pipeline for each output column\n",
    "pipelines = {}\n",
    "\n",
    "# Train a pipeline for each output column\n",
    "for output_col in output_columns:\n",
    "    pipeline = make_pipeline(\n",
    "        SimpleImputer(strategy='mean'),  # Impute missing values\n",
    "        LogisticRegression(max_iter=1000)  # Logistic regression\n",
    "    )\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train[output_col])\n",
    "    pipelines[output_col] = pipeline\n",
    "\n",
    "# Predicting the probabilities for each row in X_test\n",
    "predictions = {col: [] for col in output_columns}  # Initialize dictionary to store predictions\n",
    "\n",
    "for index, row in X_test.iterrows():\n",
    "    for output_col in output_columns:\n",
    "        # Predict the probability for the current row and output column\n",
    "        pred_prob = pipelines[output_col].predict_proba(row.values.reshape(1, -1))[0][1]\n",
    "        predictions[output_col].append(pred_prob)\n",
    "\n",
    "# Optionally, print out the predicted probabilities for the first few rows of X_test\n",
    "for i, (index, row) in enumerate(X_test.iterrows()):\n",
    "    if i >= 5:  # Limit output to first 5 rows\n",
    "        break\n",
    "    print(f\"Predictions for row {index}:\")\n",
    "    for output_col in output_columns:\n",
    "        print(f\"  {output_col}: {predictions[output_col][i]:.2%}\")\n",
    "    print()  # Newline for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure for Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phone:\n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df  # Assume data_df is a DataFrame loaded with user data\n",
    "\n",
    "    def collect_data(self, userid):\n",
    "        \"\"\"Collects a random data row for a given user.\"\"\"\n",
    "        user_data = self.data_df[self.data_df['user_id'] == userid].sample(n=1)\n",
    "        return user_data\n",
    "\n",
    "    def process_data(self, userid, model):\n",
    "        \"\"\"Processes data using a specified model.\"\"\"\n",
    "        data = self.collect_data(userid)\n",
    "        # Assuming `model` is a function passed to process the data\n",
    "        processed_data = model(data)\n",
    "        return processed_data\n",
    "\n",
    "    def send_data(self, userid, interval, server):\n",
    "        \"\"\"Periodically sends data at specified intervals.\"\"\"\n",
    "        data = self.collect_data(userid)\n",
    "        server.store_update_data(data)\n",
    "        Timer(interval, self.send_data, args=[userid, interval, server]).start()\n",
    "\n",
    "class Server:\n",
    "    def __init__(self):\n",
    "        self.storage_df = pd.DataFrame()  # Separate DataFrame for storing data\n",
    "\n",
    "    def request_data(self, phone, userid, raw=True):\n",
    "        \"\"\"Requests data from the Phone class.\"\"\"\n",
    "        if raw:\n",
    "            return phone.collect_data(userid)\n",
    "        else:\n",
    "            return phone.process_data(userid, self.process_data)  # Example: self.process_data as a placeholder\n",
    "\n",
    "    def process_data(self, data):\n",
    "        \"\"\"Processes data.\"\"\"\n",
    "        # This is a placeholder for data processing logic, which could involve ML models or other transformations\n",
    "        processed_data = data  # Simplified for demonstration\n",
    "        return processed_data\n",
    "\n",
    "    def store_update_data(self, data):\n",
    "        \"\"\"Stores or updates data in a separate DataFrame.\"\"\"\n",
    "        self.storage_df = pd.concat([self.storage_df, data], ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('ExtraSensory_Combined_User_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone = Phone(data_df)\n",
    "server = Server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid=\"81536B0A-8DBF-4D8A-AC24-9543E2E4C8E0\"\n",
    "raw_data = server.request_data(phone, userid, raw=True)\n",
    "processed_data = server.request_data(phone, userid, raw=False)\n",
    "server.store_update_data(raw_data)\n",
    "server.store_update_data(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.storage_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
