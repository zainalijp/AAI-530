{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Data Consolidation\n",
       "Getting the data from all the different files into one csv."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Game plan from here:\n",
       "new csv file for generated labels and real labels \n",
       "model that I am working on (using lstm model to predict next USER DATA & next activity prediction and create a row for it) concat into singular CSV (don't have to rename files beforehand) \n",
       "\n",
       "Two csv files ? \n",
       "\n",
       "Possibly use labels to predict next user data \n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Standard library imports\n",
       "import gzip\n",
       "import os\n",
       "import shutil\n",
       "import zipfile\n",
       "import pickle\n",
       "from threading import Timer\n",
       "\n",
       "# Data handling and numerical analysis\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "\n",
       "# Visualization libraries\n",
       "import seaborn as sns\n",
       "import matplotlib.pyplot as plt\n",
       "from IPython.display import Markdown, display\n",
       "\n",
       "# Machine Learning and Deep Learning libraries\n",
       "import tensorflow as tf\n",
       "from tensorflow.keras.models import Sequential\n",
       "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten, BatchNormalization, Reshape, Bidirectional\n",
       "from tensorflow.keras.optimizers import Adam\n",
       "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
       "from tensorflow.keras.regularizers import l2\n",
       "from tensorflow.keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator\n",
       "from tensorflow.keras.models import load_model, clone_model\n",
       "\n",
       "from keras.layers import Dense, Dropout, LSTM\n",
       "from keras.models import Sequential\n",
       "from keras.optimizers import Adam\n",
       "\n",
       "import sklearn\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
       "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
       "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
       "from sklearn.ensemble import RandomForestClassifier\n",
       "from sklearn.pipeline import make_pipeline\n",
       "from sklearn.impute import SimpleImputer\n",
       "from skmultilearn.problem_transform import ClassifierChain\n",
       "\n",
       "# PyTorch imports\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "from torch.utils.data import DataLoader, TensorDataset\n",
       "\n",
       "import warnings\n",
       "warnings.filterwarnings('ignore')\n",
       "\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "pd.set_option('display.max_rows', None)\n",
       "pd.set_option('display.max_columns', None)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Data Preparation and Model Training"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Function to unzip the dataset\n",
       "def unzip(zip_file):\n",
       "    # Extract to the directory obtained from the zip file name\n",
       "    zip_extract_to = zip_file.replace('.zip', '')\n",
       "\n",
       "    # Unzipping\n",
       "    if os.path.exists(zip_file):\n",
       "        if not os.path.exists(zip_extract_to):\n",
       "            os.makedirs(zip_extract_to)\n",
       "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
       "                zip_ref.extractall(zip_extract_to)\n",
       "            message = \"Unzipped successfully.\"\n",
       "        else:\n",
       "            message = \"Directory already exists. File might be unzipped.\"\n",
       "    else:\n",
       "        message = \"Zip file not found.\"\n",
       "\n",
       "    print(message)\n",
       "    return zip_extract_to\n",
       "\n",
       "# Function to extract CSV from .gz files\n",
       "def csv_extract(zip_extract_to):\n",
       "    # Improved variable name for the directory where the extracted files will be saved\n",
       "    unzipped_data_dir = f\"{zip_extract_to}-Unzipped\"\n",
       "\n",
       "    # Create the unzipped data directory if it does not exist\n",
       "    if not os.path.exists(unzipped_data_dir):\n",
       "        os.makedirs(unzipped_data_dir)\n",
       "\n",
       "    # Extracting .csv.gz files\n",
       "    extraction_message = \"\"\n",
       "    if os.path.exists(zip_extract_to):\n",
       "        for file in os.listdir(zip_extract_to):\n",
       "            if file.endswith('.gz'):\n",
       "                gz_file_path = os.path.join(zip_extract_to, file)\n",
       "                csv_file_path = os.path.join(unzipped_data_dir, file[:-3])  # Removing '.gz' from filename\n",
       "\n",
       "                try:\n",
       "                    with gzip.open(gz_file_path, 'rb') as f_in:\n",
       "                        with open(csv_file_path, 'wb') as f_out:\n",
       "                            shutil.copyfileobj(f_in, f_out)\n",
       "                    extraction_message += f\"Extracted {file}\\n\"\n",
       "                except Exception as e:\n",
       "                    extraction_message += f\"Error extracting {file}: {e}\\n\"\n",
       "    else:\n",
       "        extraction_message = \"Directory with .gz files not found.\"\n",
       "\n",
       "    print(extraction_message.strip())\n",
       "\n",
       "    return unzipped_data_dir\n",
       "\n",
       "# Function to combine CSVs into one DataFrame\n",
       "def make_one_csv(unzipped_data_dir, COMBINED_FILE):\n",
       "    # Combining all CSVs into one dataframe\n",
       "    combined_csv_data = pd.DataFrame()\n",
       "\n",
       "    if os.path.exists(unzipped_data_dir):\n",
       "        for file in os.listdir(unzipped_data_dir):\n",
       "            if file.endswith('.csv'):\n",
       "                file_path = os.path.join(unzipped_data_dir, file)\n",
       "                user_id = extract_user_id(file)\n",
       "\n",
       "                # Read the CSV file and add the user_id column\n",
       "                csv_data = pd.read_csv(file_path)\n",
       "                csv_data['user_id'] = user_id\n",
       "\n",
       "                # Append to the combined dataframe\n",
       "                combined_csv_data = pd.concat([combined_csv_data, csv_data], ignore_index=True)\n",
       "\n",
       "        # Check if any data has been combined\n",
       "        if not combined_csv_data.empty:\n",
       "            # Save the combined CSV data to a file\n",
       "            combined_csv_data.to_csv(COMBINED_FILE, index=False)\n",
       "            print(f\"Combined CSV file created at {COMBINED_FILE}.\")\n",
       "        else:\n",
       "            print(\"No CSV files found to combine or combined data is empty.\")\n",
       "    else:\n",
       "        print(\"Directory with unzipped CSV files not found.\")\n",
       "    return COMBINED_FILE\n",
       "\n",
       "# Function to extract user_id from filename\n",
       "def extract_user_id(filename):\n",
       "    return filename.split('.')[0]\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Main function to process and combine CSV files\n",
       "def process_and_combine_csv(zip_file):\n",
       "    COMBINED_FILE = 'ExtraSensory_Combined_User_Data.csv'\n",
       "    if not os.path.exists(COMBINED_FILE):\n",
       "        zip_extract_to = unzip(zip_file)\n",
       "        unzipped_data_dir = csv_extract(zip_extract_to)\n",
       "        make_one_csv(unzipped_data_dir, COMBINED_FILE)\n",
       "    else:\n",
       "        print('Combined file already exists.')\n",
       "\n",
       "# Example usage\n",
       "zip_file = 'ExtraSensory.per_uuid_features_labels.zip'\n",
       "process_and_combine_csv(zip_file)\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Data Exploration & Models Creation"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load the combined CSV file\n",
       "combined_csv_data = pd.read_csv('ExtraSensory_Combined_User_Data.csv')\n",
       "combined_csv_data['timestamp'] = pd.to_datetime(combined_csv_data['timestamp'], unit='s')"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Trying to understand columns\n",
       "def build_hierarchy(columns):\n",
       "    # Build a nested dictionary representing the hierarchy of columns.\n",
       "    hierarchy = {}\n",
       "    for col in columns:\n",
       "        parts = col.split(':')\n",
       "        current_level = hierarchy\n",
       "\n",
       "        for part in parts[:-1]:\n",
       "            current_level = current_level.setdefault(part, {})\n",
       "        \n",
       "        current_level[parts[-1]] = col\n",
       "\n",
       "    return hierarchy\n",
       "\n",
       "def format_hierarchy(hierarchy, indent=0):\n",
       "    # Format the hierarchy into a readable string with indentation.\n",
       "    result = \"\"\n",
       "    for key, value in hierarchy.items():\n",
       "        prefix = \"  \" * indent + \"- \"\n",
       "        if isinstance(value, dict):\n",
       "            result += f\"{prefix}{key}:\\n{format_hierarchy(value, indent + 1)}\"\n",
       "        else:\n",
       "            result += f\"{prefix} {key}\\n\"\n",
       "    return result\n",
       "\n",
       "# Building and formatting the hierarchy\n",
       "hierarchy = build_hierarchy(combined_csv_data.columns)\n",
       "formatted_hierarchy = format_hierarchy(hierarchy)\n",
       "print(formatted_hierarchy)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Model Training and Evaluation"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Splitting the dataset\n",
       "X = combined_csv_data.drop(['user_id', 'label:UNKNOWN', 'label_source'], axis=1)\n",
       "y = combined_csv_data['label:UNKNOWN']\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
       "\n",
       "# Model creation: LSTM example\n",
       "model = Sequential([\n",
       "    LSTM(50, input_shape=(X_train.shape[1], 1), return_sequences=True),\n",
       "    Dropout(0.5),\n",
       "    LSTM(50, return_sequences=False),\n",
       "    Dropout(0.5),\n",
       "    Dense(100, activation='relu'),\n",
       "    Dense(1, activation='sigmoid')\n",
       "])\n",
       "\n",
       "model.compile(optimizer=Adam(learning_rate=0.001),\n",
       "              loss='binary_crossentropy',\n",
       "              metrics=['accuracy'])\n",
       "\n",
       "# Model training\n",
       "epochs = 3\n",
       "batch_size = 32\n",
       "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
       "\n",
       "# Model evaluation\n",
       "loss = model.evaluate(X_test, y_test)\n",
       "print(f\"Test Loss: {loss:.4f}\")\n",
       "\n",
       "# Predictions\n",
       "y_pred = model.predict(X_test)\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Visualization of Training and Validation Loss"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Extracting loss and validation loss values\n",
       "training_loss = history.history['loss']\n",
       "validation_loss = history.history['val_loss']\n",
       "\n",
       "# Creating epoch numbers (starting from 1)\n",
       "epochs_range = range(1, epochs + 1)\n",
       "\n",
       "# Plotting the training and validation loss\n",
       "plt.figure(figsize=(8, 4))\n",
       "plt.plot(epochs_range, training_loss, 'bo-', label='Training Loss')\n",
       "plt.plot(epochs_range, validation_loss, 'ro-', label='Validation Loss')\n",
       "plt.title('Training and Validation Loss per Epoch')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Loss')\n",
       "plt.legend()\n",
       "\n",
       "plt.show()\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## IoT Data Collection and Processing"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "class Phone:\n",
       "    def __init__(self, data_df):\n",
       "        self.data_df = data_df  # Assume data_df is a DataFrame loaded with user data\n",
       "\n",
       "    def collect_data(self, userid):\n",
       "        \"\"\"Collects a random data row for a given user.\"\"\"\n",
       "        user_data = self.data_df[self.data_df['user_id'] == userid].sample(n=1)\n",
       "        return user_data\n",
       "\n",
       "    def process_data(self, userid, model):\n",
       "        \"\"\"Processes data using a specified model.\"\"\"\n",
       "        data = self.collect_data(userid)\n",
       "        # Assuming `model` is a function passed to process the data\n",
       "        processed_data = model(data)\n",
       "        return processed_data\n",
       "\n",
       "    def send_data(self, userid, interval, server):\n",
       "        \"\"\"Periodically sends data at specified intervals.\"\"\"\n",
       "        data = self.collect_data(userid)\n",
       "        server.store_update_data(data)\n",
       "        Timer(interval, self.send_data, args=[userid, interval, server]).start()\n",
       "\n",
       "class Server:\n",
       "    def __init__(self):\n",
       "        self.storage_df = pd.DataFrame()  # Separate DataFrame for storing data\n",
       "\n",
       "    def request_data(self, phone, userid, raw=True):\n",
       "        \"\"\"Requests data from the Phone class.\"\"\"\n",
       "        if raw:\n",
       "            return phone.collect_data(userid)\n",
       "        else:\n",
       "            return phone.process_data(userid, self.process_data)  # Example: self.process_data as a placeholder\n",
       "\n",
       "    def process_data(self, data):\n",
       "        \"\"\"Processes data.\"\"\"\n",
       "        # This is a placeholder for data processing logic, which could involve ML models or other transformations\n",
       "        processed_data = data  # Simplified for demonstration\n",
       "        return processed_data\n",
       "\n",
       "    def store_update_data(self, data):\n",
       "        \"\"\"Stores or updates data in a separate DataFrame.\"\"\"\n",
       "        self.storage_df = pd.concat([self.storage_df, data], ignore_index=True)\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load the combined CSV file\n",
       "data_df = pd.read_csv('ExtraSensory_Combined_User_Data.csv')\n",
       "# Initialize Phone and Server instances\n",
       "phone = Phone(data_df)\n",
       "server = Server()\n",
       "\n",
       "# Example usage\n",
       "userid = \"81536B0A-8DBF-4D8A-AC24-9543E2E4C8E0\"\n",
       "raw_data = server.request_data(phone, userid, raw=True)\n",
       "processed_data = server.request_data(phone, userid, raw=False)\n",
       "server.store_update_data(raw_data)\n",
       "server.store_update_data(processed_data)\n",
       "\n",
       "server.storage_df.head()\n"
      ]
     }
    ],
    "metadata":  {
        "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
        },
        "language_info": {
         "codemirror_mode": {
          "name": "ipython",
          "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.8.5"
        }
       },
       "nbformat": 4,
       "nbformat_minor": 2
      }